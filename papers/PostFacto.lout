@SysInclude { report }
@SysInclude { eq }

def @Class right name { @F {{@Sym angleleft}name{@Sym angleright}} }
macro @Sigm {@Sym sigma}
def @Sig right n { {@Sigm} @Sub n }
def @Tau right n { {@Sym tau} @Sub n }
def @Project left union right n { union{@Sym bar} @Sub n }

# some convenience definitions inside @Eq
#
import @Eq
def set
  precedence 100
  right x
{ lbrace ` x ` rbrace }

import @Eq
def sig left index { sigma sub index }

import @Eq
def sigp left index right x { index sig (x) }

import @Eq
def ta left index { tau sub index }

import @Eq
def taus
  left maxindex
  named prime {}
  named preset {}
  named inset {2 prime}
{ {1 prime} ta, preset inset ta, ..., {maxindex prime} ta }
# { {1 prime} ta, {2 prime} ta, ..., {maxindex prime} ta }

import @Eq
def proj
  precedence 50
  left coll
  right index
{ coll big non mid sub index }


@Database @Reference { refs }

@Report
@DateLine { Yes }
@Title {
Post-Facto Type Inference
}
@Author { Gabor Greif }
@Institution { @F gabor@mac.com }
@AbstractDisplay { Yes }
# @AbstractFormat { @Centre @Title @DP @Body }
@Abstract {

We explore a novel kind of type inference in
object-oriented, dynamically dispatched setting.
The inference is based on the observation that
the absence of certain semantics-dictated errors
can be exploited to draw conclusions about the
dynamic type of function call arguments. We
sketch the implementation of post-facto type
inference in the Gwydion Dylan compiler and
provide empirical data about the effects of the
resulting optimization and its applicability.
}
//

##################################
##################################
##################################
@Section
@Title { Structure of the Paper }
@Begin
@PP
In this report we first introduce the needed
concepts and vocabulary, and then describe the
type inference algorithm. A section is devoted
to the description of the implementation in the
Gwydion Dylan @F d2c compiler along with measurements
on the effectivity of the applicable optimizations.
The last three sections compare our method to
other work, explore future lines of thought
and conclude.
@End @Section

##################################
##################################
##################################
@Section
@Title { Introduction }
@Begin
@PP
We start out by providing a short informal tour
of dynamic typing in object-oriented languages.
Following that we describe a distinguishing feature of
the Dylan programming language, namely @I { multiple
dispatch } with its associated concept of @I { generic
functions }.

#################
@BeginSubSections

###########
@SubSection @Title { Dynamic Typing }
@Begin
@PP
In dynamically typed languages the compiler needs no
static knowledge of the actual type of an object. Instead,
a run-time typecheck gets inserted for every assertion
the program or the compiler makes, so that no undefined
behaviour can accidentally arise. Dynamic typing is just one
way to achieve type safeness. A type-safe program cannot
go wrong as long as enough resources are available.
Naturally, one is interested in compensating the performance
penalty that occurs due to the lack of exact information.
For some languages or implementations the compiler
is able to approximate the type of a dynamic object
by either propagating the @I { type annotations }
or by @I inferring a more restricted @I subtype for values
instead of assuming the default, the root class
{@Class object}. In this case we talk about @I
{ statically known } types. But there is no way to
generally figure out the @I { dynamic type } of an
object, since classes may be added in other libraries,
or even at runtime, outside of the compiler's know.
@End @SubSection

###########
@SubSection @Title { Multiple Dispatch }
@Begin
@PP
In contrast to the conventional @I { single receiver }
languages, which allow polymorpic behaviour only on
the distinguished receiver argument, @I { multiply dispatched }
languages such as Dylan allow an ordered set of arguments
determine behaviour in cooperation. The classical example
of the usefulness of this feature is the output of different
documents on a range of devices. In Dylan one would declare
@IndentedDisplay @F @Verbatim
{
define generic render (doc :: <document>, dev :: <device>) => ();
}
and specialize @F {method}s with either one or both arguments'
types given more precisely:
@IndentedDisplay @F @F @Verbatim
{
define method render (doc :: <html-document>, dev :: <laser-printer>) => (); ...
define method render (doc :: <pdf-document>, dev :: <dot-matrix-printer>) => (); ...
}
@End @SubSection

###########
@SubSection @Title { Generic Functions and Sealing }
@Begin
@PP
The mechanism how Dylan manages to find an appropriate
method that best approximates the dynamic types of
invocation arguments is accomplished via @I { generic
functions }. Generic functions (also GFs for brevity)
hold a set of methods internally, each having a @I signature
which tells the GF for which dynamic types of the arguments
each method becomes @I applicable.
The exact workings of this are rather arcane, but well
described in the DRM @Cite { $shalit1996 }. For our purposes we simply recognize
that the applicable methods get determined by the GF and
sorted by @I { specificity }. In this process the @I ambiguous
methods (i.e. those that are neither less nor more
specific than another method) get separated away and not
considered any more. From the remaining methods the most
specific gets called to obtain behaviour or a function result.
To obtain good performance even in a multi-dispatched setting
a compiler will try to resolve to the most-applicable method
at compile time. Naturally, being able to do this depends
on the closed world assumption. To aid the compiler in closing
the small worlds of generic functions, the Dylan language
specifies the @I library as the unit of compilation and gives
the library writer a tool called @I sealing. By using
@IndentedDisplay @F @Verbatim
{
define sealed domain render (<document>, <laser-printer>) => ();
}
the GF @F render is prohibited from accepting methods specialized
on subtypes of the given signature that originate from outside of the
library being the provider of the seal. Sealed domains also arise
implicitly by not declaring GFs as being @F { open }. In the
remaining part of this paper we assume all generic functions
mentioned as being sealed.

@End @SubSection

@EndSubSections

@End @Section


##################################
##################################
##################################
@Section
@Title { The Post-Facto Analysis }
@Begin
@PP
Our analysis centers around the observation that a successful function
call automatically implies all semantics dictated constraints w.r.t.
the function's call and return preconditions being fulfilled. To stay with
our running example from the introduction, if a call @F { render(a, b) }
returns, we can safely assume that the constraints @F { instance?(a, <document>) }
and @F { instance?(b, <device>) } hold true.

#################
@BeginSubSections

###########
@SubSection @Title { Using the Function Signature }
@Begin
@PP
In Dylan (among other dynamically typed safe languages) a
function's signature conveys two contracts:
@BulletList
   @ListItem { the caller promises to only invoke the function
with the number and types of arguments as laid down in the @I { argument
declaration } of the signature, and }
   @ListItem { the function promises to return the number and types of
values as laid down in the @I { return declaration } of the signature. }
@EndList
In the case that any of these contracts are violated, the function does
not return, but @I { signals an error }.
In summary the function call return is an evidence that the contract
by types has been fulfilled. Naturally, if the language provides means of
intercepting runtime errors (Dylan provides @F {block}s and @F {let handler}s
for this purpose) the inference domain ends at these @I { error boundaries }.
@PP This analysis is certainly widely known and exploited, e.g. by @I { type
checking } arguments before the call to the function and propagating the type-refined
expressions furtheron. Also the declared types of the returned values can be
used in the obvious way. Our contribution is the exploration of the described
principle in a multiply dispatched setting.
@End @SubSection

###########
@SubSection @Title { Determining Call Candidates }
@Begin
@PP
For sealed generic functions (more precisely sealed GF domains) the closed-world
assumption holds, and the compiler is in the possession of every method that belongs
to the GF domain, along with their signatures. This allows the compiler to rule out
any method as being not applicable where the static types of the arguments are
provably disjoint to the respective argument type of the method. What remains is the
set of possibly applicable methods. In addition the compiler is able to determine
the @I { definitive set } of methods that are provably applicable by checking that
the static types of arguments are subtypes of the respective argument types in the
signature. Leaving away the definitive set from the applicable set the @I { maybe set }
remains.
@LP @I { [nice set diagram here] }
@PP
The Dylan language semantics @Cite { $shalit1996, chapter 6 } describes that from the set of
applicable methods the one with the most specific argument types will be called
(which in turn may call @F next-method to pass control to the next specific one
for code reuse purposes).
@End @SubSection

###########
@SubSection @Title { Extracting Typing Constraints }
@Begin
@PP
When the compiler can determine exactly one most specific method that is to be called,
the widely known and applied optimization arises: the return type of the function can
be utilized to track the type of the value returned from the function call.
@PP This is not our focus however. We are interested in the amount of information that
can be extracted from the call to the GF when it is not exactly known to the compiler
which method form the maybe set will be invoked at runtime. In the terms of the previous
section the compiler infers a nonempty fixed applicable set (because of sealig), and an
empty definitive set (because its knowledge of argument types is not precise enough).
Our optimization tries to turn this vice of multiple dispatch to a virtue, nevertheless.
@PP
We observe that since all methods in the maybe set are applicable, one of them @I
{ will have been called } when"/"if the call returns. This can be stated in terms of
typing: each argument must have been an instance of the union of the
types at the corresponding position across the signatures in the maybe set, if the
function call returns @I {without signalling an error}. In the dominance
domain of this call we can assume that the above type constraint holds.
We introduce some notation to be able to formalize this a bit. Let {@Sigm}({@Tau 1}, {@Tau 2}, ..., {@Tau @I n})
denote the product type of an {@I n}-ary method's argument signature. The maybe set then
can be described as a set of distinct signatures: "{" {@Sig 1}, {@Sig 2}, ..., @Sig @I m "}".
We denote the projection operator to extract the type of argument @Eq {1 <= i <= n} from a
method signature: {@Sigm}({@Tau 1}, {@Tau 2}, ..., {@Tau @I n}) {@Sym bar} @Sub @I i = {@Tau @I i}.
@PP This allows to formulate our above notion as
@CentredNumberedDisplay @Tag { pessim } {
"{" {@Sig 1}, {@Sig 2}, ..., {@Sig @I m} "}"{@Project @I i} @Sym lessequal
{@F type-union}({@Sig 1} {@Project @I i}, ..., {@Sig @I m} {@Project @I i}).
}
One might ask
for the reason of the inequality in this formula. It simply states that
we can only expect to get an approximation. On the other hand, it gives us
some hope that we can do even @I better than the equality suggests!
@End @SubSection

###########
@SubSection @Title { Polynomial Constraints }
@Begin
@PP
Consider the following Dylan snippet:
@LP
@IndentedDisplay @F @Verbatim {

define method theory-of-everything(t == #t, v == 42) ...
define method theory-of-everything(t == #f, v == 0) ...

...
    theory-of-everything(truth, value);
    if (truth)
        format-out("there is hope: %d", value);
    else
        format-out("ask again: %d", value);
    end if;
}

Here the first argument to @F foo selects one of the
methods @F "foo{#t, 42}" @FootNote { Dylan allows for
@I { singleton types }, where the only instance of a type is
the specified one. These types are introduced by the @F singleton
function or by the syntax shown in the above snippet. }
or {@F "foo{#f, 0}"}. In the two
arms of the @F if statement a similar selection is performed:
since @F truth can only be @F "#t" or @F "#f" the first arm is
entered in the former and the second in the latter case.
Surprisingly knowing the value (or type) of @F truth, we
can deduce the {@F value}, since @F "#t" for the first argument
automatically implies @F 42 for the second (and @F "#f" for
@F truth implies @F 0 for {@F value}).


@PP
@I { [signature unions, indexed by argument. gives rise to
multiple constraints] }
@PP
We can formalize this notion as follows. At the condition of
the @F if statement the typing constraints are
@F truth @F :: "{" {@Sig 1}({@Tau @F "#t"}, {@Tau @F 42}), {@Sig 2}({@Tau @F "#f"}, {@Tau @F 0}) "}"{@Project 1}
and @F value @F :: "{" {@Sig 1}({@Tau @F "#t"}, {@Tau @F 42}), {@Sig 2}({@Tau @F "#f"}, {@Tau @F 0}) "}"{@Project 2}.
In the second leg of the @F if statement the compiler can infer the type of @F truth being {@Tau @F "#f"}.
We can now introduce the restriction operator @Sym circlemultiply to combine the two constraints and derive further
constraints for other variables:
"{" {@Sig 1}({@Tau @F "#t"}, {@Tau @F 42}), {@Sig 2}({@Tau @F "#f"}, {@Tau @F 0}) "}"{@Project 1} @Sym circlemultiply {@Tau @F "#f"}
@Sym arrowdblright "{" {@Tau @F 0}{@Project 2} "}". Here the notation {@Tau @F 0}{@Project 2} conveys the fact that
the restriction results in a new type constraint for the second factor of the tuple union being the (singleton)
type {@Tau @F 0}.
@End @SubSection

###########
@SubSection @Title { Type Algebra of Tuple Union Projections }
@Begin
@PP
We arrived at a point where it becomes possible to actually
do computations on the tuple union projections, such as
intersection, union and set difference.
@LP @I {[to be expanded]}
@LP
When deriving the type algebra of tuple union projections we only take into
consideration the case where the tuples are of the same length and the projection
is to the same component. Unless these requirements are met we just fall back
to the pessimization (@NumberOf { pessim }) and treat the projection as a simple type union.

For the remaining subsections we assume that the requirements are met.

@BeginSubSubSections
#-----------------
@SubSubSection @Title { Type Union }
@Begin
@LP
For a fixed {@I m}, {@I m}' and @I i (@Eq {1 <= i <= n}) we define "{" {@Sig 1}, {@Sig 2}, ..., {@Sig @I m} "}"{@Project @I i} @Sym union "{" {@Sig 1'}, {@Sig 2'}, ..., {@Sig {{@I m}'}} "}"{@Project @I i}
to be "{" {@Sig 1}, {@Sig 2}, ..., {@Sig @I m} {@Sig 1'}, {@Sig 2'}, ..., {@Sig {@I m}'} "}"{@Project @I i}.

Since the set of the signatures can be already viewed as a union of one-element sets of signatures, this
concatenating definition is rather obvious. It is also compatible with (@NumberOf { pessim }).
@End @SubSubSection

#-----------------
@SubSubSection @Title { Type Intersection }
@Begin
@LP
For a fixed {@I m}, {@I m}' and @I i (@Eq {1 <= i <= n}) we are to derive "{" {@Sig 1}, {@Sig 2}, ..., {@Sig @I m} "}"{@Project @I i} @Sym intersection "{" {@Sig 1'}, {@Sig 2'}, ..., {@Sig {{@I m}'}} "}"{@Project @I i} now.
First we reduce our task by considering signature sets with just one element:
"{" {@Sig 1}({@Tau 1}, ..., {@Tau @I i}, ..., {@Tau @I n}) "}"{@Project @I i} @Sym intersection "{" {@Sig 1'}({@Tau 1'}, ..., {@Tau {{@I i}'}}, ..., {@Tau {{@I n}'}}) "}"{@Project @I i}.
An argument at position @I i can only be passed to both signatures if it is an instance of the type intersection of {@Tau @I i} and {@Tau {{@I i}'}}.
Now we reintroduce several signatures at the left hand side: clearly we have to construct the union of each {@Tau @I i} @Sup @I j @Sym intersection {@Tau {{@I i}'}}.
Here @I j is denoting the index in the left signature set. Additionally considering several signatures on the right, we arrive at
the union over the full cartesian product: @Eq {big union from {j=1, "j'"=1} to {m, "m'"} tau sub i sup j intersection tau sub i sup "j'"}.

@LP
@LP
@LP


For a fixed {@I m}, {@I m}' and @I i (@Eq {1 <= i <= n}) we are to derive @Eq { set {1 sig, 2 sig, ..., m sig} proj i cap set {1' sig, 2' sig, ..., m' sig} proj i} now.
First we reduce our task by considering signature sets with just one element:
@Eq { set {1 sigp {n taus}} proj i cap set {1' sigp {n taus prime {'} preset {... ,} inset {i}}} proj i}.

@End @SubSubSection

#-----------------
@SubSubSection @Title { Type Difference }
@Begin
# @LP
# We are to derive "{" {@Sig 1}, {@Sig 2}, ..., {@Sig @I m} "}"{@Project @I i} @Char backslash "{" {@Sig 1'}, {@Sig 2'}, ..., {@Sig {{@I m}'}} "}"{@Project @I i} now.
@LP

We are to derive
@Eq { set {1 sig, 2 sig, ..., m sig} proj i setminus set {1' sig, 2' sig, ..., m' sig} proj i }
now.

Again we consider one element sets first:
@Eq { set {1 sigp {n taus}} proj i setminus set {1' sigp {n taus prime {'}}} proj i}
@End @SubSubSection
@EndSubSubSections

@End @SubSection


@EndSubSections


@End @Section


##################################
##################################
##################################
@Section
@Title { Other Inference Methods }
@Begin
@PP
propagating inferred information in
form of types.
@End @Section


##################################
##################################
##################################
@Section
@Title { Implementing Polynomial Constraints }
@Begin
@PP
Implementing the tuple-of-unions version of post-facto
inference turned out to be relatively straightforward,
since the compiler already supplies the definitive and
maybe sets needed for the analysis. @I { [a bit more on how it is done] }
The bigger challenge is to implement the union-of-tuples
approach. The idea is to introduce a new internal type
primitive, say, {@Class tuple-union-projection}, that is
able to represent the type of the @I {n}th positional
argument of a tuple-union or the type of its keyword
argument {@F m:}.

@End @Section


##################################
##################################
##################################
@Section
@Title { Measurements of Inference Effectivity }
@Begin
@PP

It is expected that our method is most productive on sparsely
type-annotated programs. In its first incarnation of the
algorithm a dozen of optimization candidates were detected
in the 75 KLOC sourcebase of @F d2c itself. Some of them could
not effectively be exploited because while the refinement
appeared in both legs of "\"if\"" nodes and the use in a node
dominated by the "\"if\"" node, @F d2c unfortunately has no mechanism
(yet) to coalesce the results of analysis to behind the "\"if\"" node.
The above number appears as disappointingly low, but we must take into
consideration that @F d2c is a well-annotated program when it
comes to types. It is expected that after implementing the
tuple-union based optimization, a whole lot more of opportunities
for type refinement open up.

@End @Section


##################################
##################################
##################################
@Section
@Title { Further Work }
@Begin
@PP
@StarList
   @ListItem { using the fact that an error happened? }
   @ListItem { exploiting the absence from other errors (out-of-range etc.)? }
   @ListItem { using the definitive set to add information? -- possibly not! }
   @ListItem { using the known ambiguities to add information?
Certain type combinations would lead to ambiguities, thus
calls could succeed where a specific method failed (e.g. type error on a keyword arg) }
   @ListItem { using the type of returned objects to
recognize (constrain) several argument types at the same time }
   @ListItem { connection to CPS? }
   @ListItem { new entrypoint that checks types (only), no checks at call site? }
   @ListItem { @F apply and argument sequence size, keywords and keyword parameters
(every 2nd is @Class symbol and @F {oneof(}@I keywords{@F )}, the following ones are of specific type) }
   @ListItem { using @F primary to ensure type disjointness (similarly getter return value
conflicts) }
   @ListItem { type algebra for different projectors possible? for different tuple length? }
   @ListItem { exponential blowup on size of type terms possible? by cascaded function calls with partially new variables? }
   @ListItem { we can do a forward analysis to predicate an almost 100% probability for the postfacto refined ssa-vars (for at least the preceding basic block, infinite loops before that could be pessimized by this) }
@EndList

@End @Section


##################################
##################################
##################################
@Section
@Title { Conclusion }
@Begin
@PP
We described a technique of stating constraints on the type of arguments passed to
multimethods. The analysis is valid only @I post-facto, i.e. after the call to
the multimethod returns. We also suggest that the analysis delivers even more precise
results if the union-of-products (polynomial) constraints are maintained throughout the
algorithm. We gave an implementation sketch of the full optimization and provided
some (disappointing) results on the effectivity of the simple-minded implementation on
a well annotated input. It remains to be seen whether the fully implemented algorithm can
significantly improve the compiler's ability to approximate the dynamic type of expressions
in Dylan programs written in a scripting-language style by a purely static analysis.
@End @Section


##################################
##################################
##################################
@Section
@Title { Thanks }
@Begin
@PP
Peter Housel, Kim Barrett, Bruce Hoult
@End @Section


##################################
##################################
##################################
@Section
@Title { Bibliography }
@Begin
@PP

@End @Section


